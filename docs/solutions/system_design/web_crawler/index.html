<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Design a web crawler | The System Design Primer</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="Design a web crawler">
<meta property="og:locale" content="en_US">
<meta name="description" content="Learn how to design large-scale systems. Prep for the system design interview.">
<meta property="og:description" content="Learn how to design large-scale systems. Prep for the system design interview.">
<link rel="canonical" href="https://phucnguyen81.github.io/system-design-primer/solutions/system_design/web_crawler/">
<meta property="og:url" content="https://phucnguyen81.github.io/system-design-primer/solutions/system_design/web_crawler/">
<meta property="og:site_name" content="The System Design Primer">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Design a web crawler">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Learn how to design large-scale systems. Prep for the system design interview.","headline":"Design a web crawler","url":"https://phucnguyen81.github.io/system-design-primer/solutions/system_design/web_crawler/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/system-design-primer/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="https://phucnguyen81.github.io/system-design-primer/feed.xml" title="The System Design Primer">
<link rel="shortcut icon" type="image/x-icon" href="/system-design-primer/assets/favicon.ico">
<link rel="stylesheet" type="text/css" href="/system-design-primer/assets/css/custom-styles.css">
<link rel="stylesheet" type="text/css" href="/system-design-primer/assets/css/custom-mermaid.css">
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/system-design-primer/">The System Design Primer</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/system-design-primer/about/">About</a><a class="page-link" href="/system-design-primer/pages/asynchronism.html">Asynchronism</a><a class="page-link" href="/system-design-primer/pages/cache.html">Cache</a><a class="page-link" href="/system-design-primer/pages/domain-name-system.html">Domain name system</a><a class="page-link" href="/system-design-primer/pages/federation.html">Federation</a><a class="page-link" href="/system-design-primer/">The System Design Primer</a><a class="page-link" href="/system-design-primer/images/">Original Images</a><a class="page-link" href="/system-design-primer/pages/key-value-store.html">Key value store</a><a class="page-link" href="/system-design-primer/pages/load-balancer.html">Load balancer</a><a class="page-link" href="/system-design-primer/pages/master-master-replication.html">Master-master replication</a><a class="page-link" href="/system-design-primer/pages/master-slave-replication.html">Master-slave replication</a><a class="page-link" href="/system-design-primer/pages/replication.html">Replication</a><a class="page-link" href="/system-design-primer/pages/reverse-proxy-web-server.html">Reverse Proxy (web server)</a><a class="page-link" href="/system-design-primer/pages/sharding.html">Sharding</a><a class="page-link" href="/system-design-primer/pages/sql-or-nosql.html">SQL or NoSQL</a><a class="page-link" href="/system-design-primer/solutions/system_design/mint/">Design Mint.com</a><a class="page-link" href="/system-design-primer/solutions/system_design/pastebin/">Design Pastebin.com</a><a class="page-link" href="/system-design-primer/solutions/system_design/query_cache/">Design a key-value cache to save the results of the most recent web server queries</a><a class="page-link" href="/system-design-primer/solutions/system_design/sales_rank/">Design Amazon’s sales rank by category feature</a><a class="page-link" href="/system-design-primer/solutions/system_design/scaling_aws/">Design a system that scales to millions of users on AWS</a><a class="page-link" href="/system-design-primer/solutions/system_design/social_graph/">Design the data structures for a social network</a><a class="page-link" href="/system-design-primer/solutions/system_design/twitter/">Design the Twitter timeline and search</a><a class="page-link" href="/system-design-primer/solutions/system_design/web_crawler/">Design a web crawler</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Design a web crawler</h1>
  </header>

  <div class="post-content">
    <h1 id="design-a-web-crawler">Design a web crawler</h1>

<p><em>Note: This document links directly to relevant areas found in the <a href="/#index-of-system-design-topics">system design topics</a> to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.</em></p>

<h2 id="step-1-outline-use-cases-and-constraints">Step 1: Outline use cases and constraints</h2>

<blockquote>
  <p>Gather requirements and scope the problem.
Ask questions to clarify use cases and constraints.
Discuss assumptions.</p>
</blockquote>

<p>Without an interviewer to address clarifying questions, we’ll define some use cases and constraints.</p>

<h3 id="use-cases">Use cases</h3>

<h4 id="well-scope-the-problem-to-handle-only-the-following-use-cases">We’ll scope the problem to handle only the following use cases</h4>

<ul>
  <li>
<strong>Service</strong> crawls a list of urls:
    <ul>
      <li>Generates reverse index of words to pages containing the search terms</li>
      <li>Generates titles and snippets for pages
        <ul>
          <li>Title and snippets are static, they do not change based on search query</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>User</strong> inputs a search term and sees a list of relevant pages with titles and snippets  the crawler generated
    <ul>
      <li>Only sketch high level components and interactions for this use case, no need to go into depth</li>
    </ul>
  </li>
  <li>
<strong>Service</strong> has high availability</li>
</ul>

<h4 id="out-of-scope">Out of scope</h4>

<ul>
  <li>Search analytics</li>
  <li>Personalized search results</li>
  <li>Page rank</li>
</ul>

<h3 id="constraints-and-assumptions">Constraints and assumptions</h3>

<h4 id="state-assumptions">State assumptions</h4>

<ul>
  <li>Traffic is not evenly distributed
    <ul>
      <li>Some searches are very popular, while others are only executed once</li>
    </ul>
  </li>
  <li>Support only anonymous users</li>
  <li>Generating search results should be fast</li>
  <li>The web crawler should not get stuck in an infinite loop
    <ul>
      <li>We get stuck in an infinite loop if the graph contains a cycle</li>
    </ul>
  </li>
  <li>1 billion links to crawl
    <ul>
      <li>Pages need to be crawled regularly to ensure freshness</li>
      <li>Average refresh rate of about once per week, more frequent for popular sites
        <ul>
          <li>4 billion links crawled each month</li>
        </ul>
      </li>
      <li>Average stored size per web page: 500 KB
        <ul>
          <li>For simplicity, count changes the same as new pages</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>100 billion searches per month</li>
</ul>

<p>Exercise the use of more traditional systems - don’t use existing systems such as <a href="http://lucene.apache.org/solr/">solr</a> or <a href="http://nutch.apache.org/">nutch</a>.</p>

<h4 id="calculate-usage">Calculate usage</h4>

<p><strong>Clarify with your interviewer if you should run back-of-the-envelope usage calculations.</strong></p>

<ul>
  <li>2 PB of stored page content per month
    <ul>
      <li>500 KB per page * 4 billion links crawled per month</li>
      <li>72 PB of stored page content in 3 years</li>
    </ul>
  </li>
  <li>1,600 write requests per second</li>
  <li>40,000 search requests per second</li>
</ul>

<p>Handy conversion guide:</p>

<ul>
  <li>2.5 million seconds per month</li>
  <li>1 request per second = 2.5 million requests per month</li>
  <li>40 requests per second = 100 million requests per month</li>
  <li>400 requests per second = 1 billion requests per month</li>
</ul>

<h2 id="step-2-create-a-high-level-design">Step 2: Create a high level design</h2>

<blockquote>
  <p>Outline a high level design with all important components.</p>
</blockquote>

<p><img src="https://i.imgur.com/xjdAAUv.png" alt="Imgur"></p>

<h2 id="step-3-design-core-components">Step 3: Design core components</h2>

<blockquote>
  <p>Dive into details for each core component.</p>
</blockquote>

<h3 id="use-case-service-crawls-a-list-of-urls">Use case: Service crawls a list of urls</h3>

<p>We’ll assume we have an initial list of <code class="language-plaintext highlighter-rouge">links_to_crawl</code> ranked initially based on overall site popularity.  If this is not a reasonable assumption, we can seed the crawler with popular sites that link to outside content such as <a href="https://www.yahoo.com/">Yahoo</a>, <a href="http://www.dmoz.org/">DMOZ</a>, etc.</p>

<p>We’ll use a table <code class="language-plaintext highlighter-rouge">crawled_links</code> to store processed links and their page signatures.</p>

<p>We could store <code class="language-plaintext highlighter-rouge">links_to_crawl</code> and <code class="language-plaintext highlighter-rouge">crawled_links</code> in a key-value <strong>NoSQL Database</strong>.  For the ranked links in <code class="language-plaintext highlighter-rouge">links_to_crawl</code>, we could use <a href="https://redis.io/">Redis</a> with sorted sets to maintain a ranking of page links.  We should discuss the <a href="/pages/sql-or-nosql">use cases and tradeoffs between choosing SQL or NoSQL</a>.</p>

<ul>
  <li>The <strong>Crawler Service</strong> processes each page link by doing the following in a loop:
    <ul>
      <li>Takes the top ranked page link to crawl
        <ul>
          <li>Checks <code class="language-plaintext highlighter-rouge">crawled_links</code> in the <strong>NoSQL Database</strong> for an entry with a similar page signature
            <ul>
              <li>If we have a similar page, reduces the priority of the page link
                <ul>
                  <li>This prevents us from getting into a cycle</li>
                  <li>Continue</li>
                </ul>
              </li>
              <li>Else, crawls the link
                <ul>
                  <li>Adds a job to the <strong>Reverse Index Service</strong> queue to generate a <a href="https://en.wikipedia.org/wiki/Search_engine_indexing">reverse index</a>
</li>
                  <li>Adds a job to the <strong>Document Service</strong> queue to generate a static title and snippet</li>
                  <li>Generates the page signature</li>
                  <li>Removes the link from <code class="language-plaintext highlighter-rouge">links_to_crawl</code> in the <strong>NoSQL Database</strong>
</li>
                  <li>Inserts the page link and signature to <code class="language-plaintext highlighter-rouge">crawled_links</code> in the <strong>NoSQL Database</strong>
</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Clarify with your interviewer how much code you are expected to write</strong>.</p>

<p><code class="language-plaintext highlighter-rouge">PagesDataStore</code> is an abstraction within the <strong>Crawler Service</strong> that uses the <strong>NoSQL Database</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PagesDataStore</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">db</span><span class="p">);</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">db</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">add_link_to_crawl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
        <span class="s">"""Add the given link to `links_to_crawl`."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">remove_link_to_crawl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
        <span class="s">"""Remove the given link from `links_to_crawl`."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">reduce_priority_link_to_crawl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
        <span class="s">"""Reduce the priority of a link in `links_to_crawl` to avoid cycles."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">extract_max_priority_page</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Return the highest priority link in `links_to_crawl`."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">insert_crawled_link</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">signature</span><span class="p">):</span>
        <span class="s">"""Add the given link to `crawled_links`."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">crawled_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signature</span><span class="p">):</span>
        <span class="s">"""Determine if we've already crawled a page matching the given signature"""</span>
        <span class="p">...</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Page</code> is an abstraction within the <strong>Crawler Service</strong> that encapsulates a page, its contents, child urls, and signature:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Page</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">contents</span><span class="p">,</span> <span class="n">child_urls</span><span class="p">,</span> <span class="n">signature</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">url</span> <span class="o">=</span> <span class="n">url</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">contents</span> <span class="o">=</span> <span class="n">contents</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">child_urls</span> <span class="o">=</span> <span class="n">child_urls</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Crawler</code> is the main class within <strong>Crawler Service</strong>, composed of <code class="language-plaintext highlighter-rouge">Page</code> and <code class="language-plaintext highlighter-rouge">PagesDataStore</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Crawler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_store</span><span class="p">,</span> <span class="n">reverse_index_queue</span><span class="p">,</span> <span class="n">doc_index_queue</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span> <span class="o">=</span> <span class="n">data_store</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reverse_index_queue</span> <span class="o">=</span> <span class="n">reverse_index_queue</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">doc_index_queue</span> <span class="o">=</span> <span class="n">doc_index_queue</span>

    <span class="k">def</span> <span class="nf">create_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">page</span><span class="p">):</span>
        <span class="s">"""Create signature based on url and contents."""</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">crawl_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">page</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">page</span><span class="p">.</span><span class="n">child_urls</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">add_link_to_crawl</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        <span class="n">page</span><span class="p">.</span><span class="n">signature</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_signature</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">remove_link_to_crawl</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">url</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">insert_crawled_link</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">page</span><span class="p">.</span><span class="n">signature</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">crawl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">page</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">extract_max_priority_page</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">page</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">crawled_similar</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">signature</span><span class="p">):</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">data_store</span><span class="p">.</span><span class="n">reduce_priority_link_to_crawl</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">url</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">crawl_page</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="handling-duplicates">Handling duplicates</h3>

<p>We need to be careful the web crawler doesn’t get stuck in an infinite loop, which happens when the graph contains a cycle.</p>

<p><strong>Clarify with your interviewer how much code you are expected to write</strong>.</p>

<p>We’ll want to remove duplicate urls:</p>

<ul>
  <li>For smaller lists we could use something like <code class="language-plaintext highlighter-rouge">sort | unique</code>
</li>
  <li>With 1 billion links to crawl, we could use <strong>MapReduce</strong> to output only entries that have a frequency of 1</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RemoveDuplicateUrls</span><span class="p">(</span><span class="n">MRJob</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">mapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">line</span><span class="p">,</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">reducer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">key</span><span class="p">,</span> <span class="n">total</span>
</code></pre></div></div>

<p>Detecting duplicate content is more complex.  We could generate a signature based on the contents of the page and compare those two signatures for similarity.  Some potential algorithms are <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a> and <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>.</p>

<h3 id="determining-when-to-update-the-crawl-results">Determining when to update the crawl results</h3>

<p>Pages need to be crawled regularly to ensure freshness.  Crawl results could have a <code class="language-plaintext highlighter-rouge">timestamp</code> field that indicates the last time a page was crawled.  After a default time period, say one week, all pages should be refreshed.  Frequently updated or more popular sites could be refreshed in shorter intervals.</p>

<p>Although we won’t dive into details on analytics, we could do some data mining to determine the mean time before a particular page is updated, and use that statistic to determine how often to re-crawl the page.</p>

<p>We might also choose to support a <code class="language-plaintext highlighter-rouge">Robots.txt</code> file that gives webmasters control of crawl frequency.</p>

<h3 id="use-case-user-inputs-a-search-term-and-sees-a-list-of-relevant-pages-with-titles-and-snippets">Use case: User inputs a search term and sees a list of relevant pages with titles and snippets</h3>

<ul>
  <li>The <strong>Client</strong> sends a request to the <strong>Web Server</strong>, running as a <a href="/pages/reverse-proxy-web-server">reverse proxy</a>
</li>
  <li>The <strong>Web Server</strong> forwards the request to the <strong>Query API</strong> server</li>
  <li>The <strong>Query API</strong> server does the following:
    <ul>
      <li>Parses the query
        <ul>
          <li>Removes markup</li>
          <li>Breaks up the text into terms</li>
          <li>Fixes typos</li>
          <li>Normalizes capitalization</li>
          <li>Converts the query to use boolean operations</li>
        </ul>
      </li>
      <li>Uses the <strong>Reverse Index Service</strong> to find documents matching the query
        <ul>
          <li>The <strong>Reverse Index Service</strong> ranks the matching results and returns the top ones</li>
        </ul>
      </li>
      <li>Uses the <strong>Document Service</strong> to return titles and snippets</li>
    </ul>
  </li>
</ul>

<p>We’ll use a public <a href="/#representational-state-transfer-rest"><strong>REST API</strong></a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ curl https://search.com/api/v1/search?query=hello+world
</code></pre></div></div>

<p>Response:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "title": "foo's title",
    "snippet": "foo's snippet",
    "link": "https://foo.com",
},
{
    "title": "bar's title",
    "snippet": "bar's snippet",
    "link": "https://bar.com",
},
{
    "title": "baz's title",
    "snippet": "baz's snippet",
    "link": "https://baz.com",
},
</code></pre></div></div>

<p>For internal communications, we could use <a href="/#remote-procedure-call-rpc">Remote Procedure Calls</a>.</p>

<h2 id="step-4-scale-the-design">Step 4: Scale the design</h2>

<blockquote>
  <p>Identify and address bottlenecks, given the constraints.</p>
</blockquote>

<p><img src="https://i.imgur.com/bWxPtQA.png" alt="Imgur"></p>

<p><strong>Important: Do not simply jump right into the final design from the initial design!</strong></p>

<p>State you would 1) <strong>Benchmark/Load Test</strong>, 2) <strong>Profile</strong> for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See <a href="/system-design-primer/solutions/system_design/scaling_aws/">Design a system that scales to millions of users on AWS</a> as a sample on how to iteratively scale the initial design.</p>

<p>It’s important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a <strong>Load Balancer</strong> with multiple <strong>Web Servers</strong>?  <strong>CDN</strong>?  <strong>Master-Slave Replicas</strong>?  What are the alternatives and <strong>Trade-Offs</strong> for each?</p>

<p>We’ll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.</p>

<p><em>To avoid repeating discussions</em>, refer to the following <a href="/#index-of-system-design-topics">system design topics</a> for main talking points, tradeoffs, and alternatives:</p>

<ul>
  <li><a href="/pages/domain-name-system">DNS</a></li>
  <li><a href="/#load-balancer">Load balancer</a></li>
  <li><a href="/pages/load-balancer#horizontal-scaling">Horizontal scaling</a></li>
  <li><a href="/pages/reverse-proxy-web-server">Web server (reverse proxy)</a></li>
  <li><a href="/#application-layer">API server (application layer)</a></li>
  <li><a href="/pages/cache">Cache</a></li>
  <li><a href="/#nosql">NoSQL</a></li>
  <li><a href="/#consistency-patterns">Consistency patterns</a></li>
  <li><a href="/#availability-patterns">Availability patterns</a></li>
</ul>

<p>Some searches are very popular, while others are only executed once.  Popular queries can be served from a <strong>Memory Cache</strong> such as Redis or Memcached to reduce response times and to avoid overloading the <strong>Reverse Index Service</strong> and <strong>Document Service</strong>.  The <strong>Memory Cache</strong> is also useful for handling the unevenly distributed traffic and traffic spikes.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href="/#latency-numbers-every-programmer-should-know">1</a></sup></p>

<p>Below are a few other optimizations to the <strong>Crawling Service</strong>:</p>

<ul>
  <li>To handle the data size and request load, the <strong>Reverse Index Service</strong> and <strong>Document Service</strong> will likely need to make heavy use sharding and federation.</li>
  <li>DNS lookup can be a bottleneck, the <strong>Crawler Service</strong> can keep its own DNS lookup that is refreshed periodically</li>
  <li>The <strong>Crawler Service</strong> can improve performance and reduce memory usage by keeping many open connections at a time, referred to as <a href="https://en.wikipedia.org/wiki/Connection_pool">connection pooling</a>
    <ul>
      <li>Switching to <a href="/#user-datagram-protocol-udp">UDP</a> could also boost performance</li>
    </ul>
  </li>
  <li>Web crawling is bandwidth intensive, ensure there is enough bandwidth to sustain high throughput</li>
</ul>

<h2 id="additional-talking-points">Additional talking points</h2>

<blockquote>
  <p>Additional topics to dive into, depending on the problem scope and time remaining.</p>
</blockquote>

<h3 id="sql-scaling-patterns">SQL scaling patterns</h3>

<ul>
  <li><a href="/pages/master-slave-replication">Read replicas</a></li>
  <li><a href="/pages/federation">Federation</a></li>
  <li><a href="/pages/sharding">Sharding</a></li>
  <li><a href="/#denormalization">Denormalization</a></li>
  <li><a href="/#sql-tuning">SQL Tuning</a></li>
</ul>

<h4 id="nosql">NoSQL</h4>

<ul>
  <li><a href="/pages/key-value-store">Key-value store</a></li>
  <li><a href="/#document-store">Document store</a></li>
  <li><a href="/#wide-column-store">Wide column store</a></li>
  <li><a href="/#graph-database">Graph database</a></li>
  <li><a href="/pages/sql-or-nosql">SQL vs NoSQL</a></li>
</ul>

<h3 id="caching">Caching</h3>

<ul>
  <li>Where to cache
    <ul>
      <li><a href="/#client-caching">Client caching</a></li>
      <li><a href="/#cdn-caching">CDN caching</a></li>
      <li><a href="/#web-server-caching">Web server caching</a></li>
      <li><a href="/#database-caching">Database caching</a></li>
      <li><a href="/#application-caching">Application caching</a></li>
    </ul>
  </li>
  <li>What to cache
    <ul>
      <li><a href="/#caching-at-the-database-query-level">Caching at the database query level</a></li>
      <li><a href="/#caching-at-the-object-level">Caching at the object level</a></li>
    </ul>
  </li>
  <li>When to update the cache
    <ul>
      <li><a href="/pages/cache-aside">Cache-aside</a></li>
      <li><a href="/#write-through">Write-through</a></li>
      <li><a href="/#write-behind-write-back">Write-behind (write-back)</a></li>
      <li><a href="/#refresh-ahead">Refresh ahead</a></li>
    </ul>
  </li>
</ul>

<h3 id="asynchronism-and-microservices">Asynchronism and microservices</h3>

<ul>
  <li><a href="/pages/asynchronism/#message-queues">Message queues</a></li>
  <li><a href="/pages/asynchronism/#task-queues">Task queues</a></li>
  <li><a href="/pages/asynchronism/#back-pressure">Back pressure</a></li>
  <li><a href="/#microservices">Microservices</a></li>
</ul>

<h3 id="communications">Communications</h3>

<ul>
  <li>Discuss tradeoffs:
    <ul>
      <li>External communication with clients - <a href="/#representational-state-transfer-rest">HTTP APIs following REST</a>
</li>
      <li>Internal communications - <a href="/#remote-procedure-call-rpc">RPC</a>
</li>
    </ul>
  </li>
  <li><a href="/#service-discovery">Service discovery</a></li>
</ul>

<h3 id="security">Security</h3>

<p>Refer to the <a href="/#security">security section</a>.</p>

<h3 id="latency-numbers">Latency numbers</h3>

<p>See <a href="/#latency-numbers-every-programmer-should-know">Latency numbers every programmer should know</a>.</p>

<h3 id="ongoing">Ongoing</h3>

<ul>
  <li>Continue benchmarking and monitoring your system to address bottlenecks as they come up</li>
  <li>Scaling is an iterative process</li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/system-design-primer/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The System Design Primer</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The System Design Primer</li>
<li><a class="u-email" href="mailto:phucknguyen@gmail.com">phucknguyen@gmail.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list">
<li><a href="https://github.com/phucnguyen81"><svg class="svg-icon"><use xlink:href="/system-design-primer/assets/minima-social-icons.svg#github"></use></svg> <span class="username">phucnguyen81</span></a></li>
<li><a href="https://www.twitter.com/phucknguyen"><svg class="svg-icon"><use xlink:href="/system-design-primer/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">phucknguyen</span></a></li>
</ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Learn how to design large-scale systems. Prep for the system design interview.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
